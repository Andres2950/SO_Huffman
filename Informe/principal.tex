% !TeX spellcheck = es
\documentclass{report}
\usepackage[utf8]{inputenc}

% Títulos automáticos en español
\usepackage[spanish]{babel}

% Soporte para buenas urls e hipervínculos entre secciones
\usepackage{hyperref}

% Citas y referencias en formato APA
% Si quiere las citas y referencias en IEEE comente esta línea
\usepackage{apacite}

% Imágenes y figuras
\usepackage{graphicx}

% Código fuente con números de línea
\usepackage{listings}
% Puede cambiar el lenguaje de código fuente
% https://www.overleaf.com/learn/latex/code_listing#Supported_languages
\usepackage{multirow}


\lstset{
    language=C,
    basicstyle=\footnotesize,
    numbers=left,
    stepnumber=1,
    showstringspaces=false,
    tabsize=1,
    breaklines=true,
    breakatwhitespace=false,
}


\def \unidad{Nombre de la escuela o unidad}
\def \programa{Nombre del programa o carrera}
\def \curso{Código - Nombre del curso}
\def \titulo{Título del Informe}
\def \subtitulo {Subtítulo}
\def \autores{
    Nombre del primer autor\\
    Correo electrónico\\
    Carnet\\
    
    \vspace{0.5cm}
    
    Nombre del segundo autor\\
    Correo electrónico\\
    Carnet
}
\def \fecha{Febrero 2021}
\def \lugar{
    Provincia, 
    Costa Rica
}

% Inicia el documento 
\begin{document}

% Inserta la portada del documento
\input{portada}

\tableofcontents

\chapter{Introducción}\label{intro}

Para este proyecto se implementó un compresor (y decompresor) de archivos de texto utilizando el algoritmo de Huffman.
Este toma un directorio con archivos de texto y los comprime en un solo archivo usando binario, al descomprimir, se restaura el directorio y los archivos.
Este sistema consiste en dos partes principales,  programa que comprime y el programa que comprime.
Ambos ofrecen tres modos de ejecución: serial, concurrente y paralelo.

\section {Proyecto Gutenberg}

Este proyecto fue probado con los top 100 libros de los últimos 30 días del proyecto Gutenberg.
Este proyecto, fundado por Michael  Hart, consiste en una colección de libros electrónicos gratuitos, que son versiones digitalizadas de libros que existen en físicamente \cite{gutenberg}.
Esta colección ofrece múltiples formatos para descargsr y leer libros, entre estos texto con formato UTF-8, el cual fue usado para este proyecto.

\section{Algoritmo de Huffman}

El algoritmo de Huffman es un algoritmo de compresión de texto, creado por David Huffman en 1952. Este fue creado ya que su profesor ofreció a sus estudiantes dos opciones: hacer el examen final, o escribir un paper sobre cómo encontrar el código óptimo para todos los símbolo en un texto. Lo que su profesor no explicó,  es que este problema ers un problema abierto y que él mismo estaba trabajando en él. Luego de pensar por mucho tiempo,  Huffman inventó un algoritmo que utiliza un árbol binario para encontrear dichos códigos óptimos. 

%https://old.maa.org/press/periodicals/convergence/discovery-of-huffman-codes 

Lo primero para entender este algoritmo es entender que las computadoras guardsn información de texto en formatos como ASCII o UNICODE, cada símbolo dentro de estos estándares tienen un código único que permite a la computadora diferenciarlos entre sí. El problema con estos estándares es que se le asigna la misma cantidad de bytes a todos los símbolos,  esto significa que una letra muy común como la A, tiene el mismo tamaño que una menos común,  como la Z. Idealmente las letras más común deberían tener un tamaño más pequeño. 

El algoritmo de Huffman busca compactar el texto, encontrando los símbolos más comunes en un texto y reduciendo el tamaño que toma em guardarlos. Para esto se cuenta la cantidad de símbolos en el texto y a cada uno se les asigna un código en binario, entre más común el simbolo más corto su código. Estos codigos se arman en un arbol binario, cada nodo interno teniendo la posibilidad de conectarse con un hijo por medio de un 1 o un 0, al recorrer un camino específico del arbol desde la raíz hasta una hoja, se forma el código que representa un símbolo del texto. 

%https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1216/assignments/7-huffman/huffman_background.html 


\section {Algoritmos de compresión con y sin pérdida}

Huffman es un algoritmo de compresión sin pérdida, esto significa que al comprimirse no se pierde contenido alguno del archivo.
La compresión sin pérdida remueve metafata innecesaria en el archivo, que luego puede ser reconstruida para su uso.
Esto es útil cuando se tiene archivos que paran de terne sentido para el usuario si se pierde su contenido, como archivos de texto.

Como su nombre lo explica, un algoritmo de compresión con pérdida, es un algoritmo que comprime el tamaño del archivo borrando información permanentemente.
Este tipo de información puede ser útil cuando el archivo sigue teniendo sentido para el usuario incluso si se perdió parte del contenido binario.
Un ejemplo de esto es una imagen,  al comprimirse es posible que esta sea un poco más borrosa que antes, pero esta sigue teniendo sentido y en el mejor caso, la diferencia no puede ser vista por el ojo humano.

%https://www.geeksforgeeks.org/computer-networks/difference-between-lossy-compression-and-lossless-compression/ 

\section{Paralelismo y concurrencia}

Concurrencia es cuando se están trabajando múltiples tareas al mismo tiempo, pero no necesariamente correrlas al mimo tiempo.
Cada tarea concurrente, toma turnos en mismo procesador. Esto es útil ya que es posible que cada tarea no solo necesite tiempo del procesador, si no que es posible que necesite otros servicios, como I/O, por lo que hace que se administre mejor el tiempo que utiliza cada hilo en el procesador. 

Paralelismo, es cuando se ejecutan tareas al mismo tiempo, en distintods procesadores. 
Esto permite que un solo programa pueda aprovechar más su uso del procesador, teniendo varios programas hijo que se encargan de subtareas.

La gran diferencia entre estos es que la concurrencia trata sobre manejar múltiples computaciones manejando el uso del CPU, mientras que el paralelismo corre múltiples computaciones al mismo tiempo.
Se puede notar que la concurrencia no es realmente que los hilos corren al mismo tiempo, si no que se intercalan los usos del CPU, dándo la ilusión de que corren al mismo tiempo.

%https://www.geeksforgeeks.org/operating-systems/difference-between-concurrency-and-parallelism/


\chapter {Desarrollo}
\section{Discusión}
\subsection{Resultados de las pruebas}
\subsubsection{Metodología de las pruebas}

Las pruebas que se realizaron tenían el objetivo de obtener el tiempo de ejecución del programa o una parte de este. Para lograr esto se decidió tomar marcas de tiempo en el código al inicio y al final de la parte que se quería probar. Se utilizó el header time.h tomar los tiempos. El tiempo de ejecución  se imprime en pantalla como la diferencia entre las dos marcas de tiempo.

Se realizaron pruebas en distintas partes de los programas y cada prueba se realizó en las 3 implementaciones diferentes. En total se probó el sistema completo del compresor y sus subsistemas de lectura y compresión; y el sistema completo del descompresor y su subsistema de descompresión y escritura.

Cada prueba se ejecutó 50 veces de forma independiente. La tabla de resultados contiene el promedio de las 50 pruebas.

\subsubsection{Tabla de resultados}

\begin{center}
	\begin{tabular}{|c|c|c|c|}		
		
\hline
\multicolumn{4}{|c|}{Tiempo de Ejecución (milisegundos)} \\
\hline
 Subsistema& \multicolumn{3}{|c|}{Versión} \\
 \hline
 & Serial & Concurrente & Paralelo\\
 \hline
huff(completo) & 5360& 2226 & 2036\\
 \hline
huff(lectura) &  36 & 25 & 19\\
 \hline
 huff(compresión) &  5168 & 1957 & 1681\\
 \hline
 dehuff(completo) & 5189 & 2092 & 1855\\
 \hline
 dehuff(descompresión y escritura) & 5146 & 1968 & 1629\\
 \hline
 
	\end{tabular}
\end{center}

\subsection{Análisis de aceleración}

\subsubsection{Metodología de Análisis}

Gene Amdahl (1922–2015) fue un informático especializado en arquitectura de hardware. Su contribución más conocida a la informática es su ley homónima, propuesta en 1967. 

La ley de Amdahl modela cuánto puede mejorarse el rendimiento del sistema al paralelizar el código. Muestra que, incluso con una potencia de cálculo infinita, las partes secuenciales limitan la aceleración. Esta ley ayuda a los desarrolladores a estimar las mejoras de rendimiento y a optimizar las secciones de código de alto impacto. La fórmula que representa la ley es la siguiente:



\begin{displaymath}	
	{T_{m} = {T_{a} \cdot \left((1 - F_{m}) + {F_{m} \over A_{m}}\right)}}
\end{displaymath}

Esta fórmula puede reescribirse para obtener la aceleración de un sistema completo luego de que se optimiza un subsistema. La formula sería la siguiente:

\begin{displaymath}	
	A = {1 \over (1 - F_{m}) + {F_{m} \over A_{m}}}	
\end{displaymath}
Donde:\\
$A$, es la aceleración o ganancia en velocidad conseguida en el sistema completo debido a la mejora de uno de sus subsistemas.\\
$A_{m}$, es el factor de mejora que se ha introducido en el subsistema mejorado.\\
$F_{m} $, es la fracción de tiempo que el sistema utiliza el subsistema mejorado.\\

La ley de Amdahl muestra que hay un limite en la aceleración posible que se puede obtener optimizando un programa. La siguiente re escritura de la fórmula nos da la aceleración máxima teórica para un sistema cuando N tiende a infinito.

	\begin{displaymath}	
		A = {2 \cdot \left 1 \over  S + {1-S \over N}}
	\end{displaymath}
Donde:\\
$A$, es la aceleración máxima conseguida en el sistema completo debido a la mejora de uno de sus subsistemas.\\
$S$, es la fracción del programa que es secuencial.\\
$N$,  el factor de mejora.\\

En las próximas secciones se realiza el cálculo par obtener la mejora de los programas de compresión y descompresión luego de haber sido optimizados mediante paralelización y concurrencia. Además se hace una discusión sobre las ganancias obtenidas a través de los diferentes métodos.

\subsubsection{Aceleración del compresor(huff)}

La ejecución del compresor se puede dividir en 3 fases: lectura, creación del árbol huffman, compresión y escritura. La etapa de lectura es donde se lee cada uno de los archivos del directorio fuente, esta etapa puede ser paralelizada.  La etapa de creación del  árbol huffman es donde se junta todo el texto para crear el diccionario con el código huffman, esta etapa no puede ser paralelizada. La etapa de compresión corresponde a la transformación de texto plano a codigo huffman de cada uno de los archivos, esta etapa puede ser paralelizada. La etapa de escritura es donde se escribe el código de cada archivo a un documento binario, esta etapa puede ser paralelizada pero se decidió no hacerlo.

La fracción del programa que abarca cada etapa sería: lectura(0.06), compresión(0.964), creación de árbol y escritura(0.02).  Lo que quiere decir que un 98\% del programa puede paralelizarse, esto implica que la aceleración máxima teórica del programa sería de 100.

La aceleración máxima que se logra obtener mediante el uso de hilos es de 2.341 y mediante el uso de fork es de 2.549. 

Ambas implementaciones mejoran drásticamente el tiempo de ejecución del programa. Ninguno de los dos se acercan al máximo teórico debido al overhead que tiene la paralelización, además el compresor cuenta con un cuello de botella en la etapa de creación del árbol huffman que se encuentra justo entre las dos etapas que se paralelizaron, lo que obliga el programa  a paralelizarse y serializarse dos veces.

Se puede observar que la implementación con paralela  fork es ligeramente más rápida que la  concurrente con hilos.

\subsubsection{Aceleración del descompresor(dehuff)}

La ejecución del descompresor se puede dividir en 2 fases: lectura del diccionario y reconstrucción del árbol, y descompresión y escritura. La primera etapa es donde se lee el inicio del archivo comprimido donde se encuentra el diccionario de códigos y se reconstruye el árbol en base a este, esta etapa no puede paralelizarse. La última etapa corresponde a la lectura del codigo de cada archivo, su descompresión y escritura, esta etapa puede ser paralelizada. 

La fracción del programa que abarca cada etapa sería: lectura de diccionario y reconstrucción de árbol (0.01), y descompresión y escritura(0.99).  Lo que quiere decir que un 99\% del programa puede paralelizarse, esto implica que la aceleración máxima teórica del programa sería de 200.

La aceleración máxima que se logra obtener mediante el uso de hilos es de 2.444 y mediante el uso de fork es de 2.727 

Ambas implementaciones mejoran drásticamente el tiempo de ejecución del programa. Ninguno de los dos se acercan al máximo teórico debido al overhead que tiene la paralelización.

 Se puede resaltar que el compresor y el descompresor tienen una fracción virtualmente idéntica de sus sistemas que fue paralelizado. Sin embargo, el descompresor obtuvo una mejor aceleración, indiferente de la implementación. Esto puede deberse a que descompresor realiza todas las tareas paralelas en el mismo punto en el código, sin ningún cuello de botella.

Se puede observar que la implementación paralela con fork es ligeramente más rápida que la  concurrente con hilos.



% Estilo de bibliografía APA
% Si quiere usar el estilo IEEE comente esta línea
\bibliographystyle{apacite}

% Descomente esta línea para usar el estilo de bibliografía IEEE
%\bibliographystyle{ieeetr}
\bibliography{referencias}

\end{document}
